{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸš— Driver Behavior Classification: Complete Pipeline\n",
    "\n",
    "**Author:** Reza Mirzaeifard\n",
    "**Date:** December 2025\n",
    "**Dataset:** UAH-DriveSet (Real-world driving telemetry)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **complete end-to-end pipeline** for driver behavior classification:\n",
    "\n",
    "1. **Raw Data Exploration** - Understanding sensor data (GPS, Accelerometer)\n",
    "2. **Feature Engineering** - Extracting meaningful features from raw sensors\n",
    "3. **Exploratory Data Analysis** - Visualizing patterns and distributions\n",
    "4. **Model Training** - Comparing 16+ classification algorithms\n",
    "5. **Deep Learning** - CNN with PyTorch\n",
    "6. **Model Evaluation** - Confusion matrices, feature importance\n",
    "\n",
    "### Problem Statement\n",
    "**Predict driver behavior from smartphone sensors: NORMAL / DROWSY / AGGRESSIVE**\n",
    "\n",
    "### Dataset: UAH-DriveSet\n",
    "| Attribute | Value |\n",
    "|-----------|-------|\n",
    "| **Source** | University of AlcalÃ¡ (naturalistic driving) |\n",
    "| **Drivers** | 6 drivers (D1-D6) |\n",
    "| **Behaviors** | 3 classes (Normal, Drowsy, Aggressive) |\n",
    "| **Road types** | Motorway, Secondary |\n",
    "| **Trips** | ~40 trips with raw sensor data |\n",
    "| **Sensors** | GPS (1Hz), Accelerometer (~50Hz) |\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|-----------|\n",
    "| **Raw features only** | Avoid circular logic from pre-computed scores |\n",
    "| **D6 held-out** | Test generalization to completely new drivers |\n",
    "| **Trip-level aggregates** | Practical for variable-length trips |\n",
    "| **Multiple models** | Find best accuracy vs interpretability tradeoff |\n",
    "\n",
    "---\n"
   ],
   "id": "771a4dfc93f94b87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clear stale imports and setup\n",
    "import sys\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if mod.startswith('src'):\n",
    "        del sys.modules[mod]\n"
   ],
   "id": "3f6a81d261af941a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from src.data import (\n",
    "    get_all_trips,\n",
    "    load_raw_gps,\n",
    "    load_raw_accelerometer,\n",
    "    load_inertial_events,\n",
    "    extract_raw_features,\n",
    "    build_raw_dataset,\n",
    "    compute_acceleration_magnitude,\n",
    "    split_by_driver,\n",
    ")\n",
    "from src.models import CNNClassifier, plot_cnn_training_history\n",
    "from src.visualization import setup_style\n",
    "from src.utils import print_success, print_header\n",
    "\n",
    "setup_style()\n",
    "print_success(\"All imports successful!\")\n"
   ],
   "id": "f263158599d6db81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 1. Understanding Raw Sensor Data\n",
    "\n",
    "### What We're Doing\n",
    "We explore the raw sensor files to understand how driving events are captured.\n",
    "\n",
    "### Sensor Data Sources\n",
    "| Sensor | Frequency | Data |\n",
    "|--------|-----------|------|\n",
    "| **GPS** | 1 Hz | Speed, coordinates, heading (course) |\n",
    "| **Accelerometer** | ~50 Hz | 3-axis acceleration (X, Y, Z) in g-forces |\n",
    "\n",
    "### Phone Orientation (Landscape Mount)\n",
    "- **X-axis (Longitudinal)**: Forward/backward â†’ Braking (negative) / Acceleration (positive)\n",
    "- **Y-axis (Lateral)**: Left/right â†’ Turning events\n",
    "- **Z-axis (Vertical)**: Up/down â†’ Road bumps, inclination\n"
   ],
   "id": "ab4fe61941b45288"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_dir = project_root / 'data' / 'UAH-DRIVESET-v1'\n",
    "\n",
    "# Get all trips\n",
    "trips = get_all_trips(data_dir)\n",
    "print(f\"ðŸ“Š Found {len(trips)} trips\")\n",
    "print(f\"   Drivers: {sorted(set(t['driver'] for t in trips))}\")\n",
    "print(f\"   Behaviors: {sorted(set(t['behavior'] for t in trips))}\")\n",
    "\n",
    "# Count by behavior\n",
    "behavior_counts = {}\n",
    "for t in trips:\n",
    "    behavior_counts[t['behavior']] = behavior_counts.get(t['behavior'], 0) + 1\n",
    "print(f\"   Distribution: {behavior_counts}\")\n"
   ],
   "id": "fd7c96a4c3adba58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Visualizing Raw Accelerometer Data\n",
    "\n",
    "Let's load a sample trip and visualize the raw sensor data to understand event detection.\n"
   ],
   "id": "562eb9ce37dfcb6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load sample AGGRESSIVE trip (has more events to visualize)\n",
    "sample_trip = [t for t in trips if t['behavior'] == 'AGGRESSIVE'][0]\n",
    "print(f\"Loading: {sample_trip['driver']} - {sample_trip['behavior']} - {sample_trip['road_type']}\")\n",
    "\n",
    "acc = load_raw_accelerometer(sample_trip['path'])\n",
    "gps = load_raw_gps(sample_trip['path'])\n",
    "\n",
    "if acc is not None:\n",
    "    acc['acc_magnitude'] = compute_acceleration_magnitude(\n",
    "        acc['acc_x_kf'].values, acc['acc_y_kf'].values, acc['acc_z_kf'].values\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    # X-axis (Braking/Acceleration)\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(acc['timestamp'][:500], acc['acc_x'][:500], alpha=0.4, label='Raw', color='lightblue')\n",
    "    ax.plot(acc['timestamp'][:500], acc['acc_x_kf'][:500], label='Kalman Filtered', color='blue', linewidth=1.5)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=-0.3, color='red', linestyle=':', alpha=0.7, label='Brake threshold')\n",
    "    ax.set_title('X-Axis: Longitudinal (Braking/Acceleration)', fontweight='bold')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Acceleration (g)')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "    # Y-axis (Turning)\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(acc['timestamp'][:500], acc['acc_y'][:500], alpha=0.4, label='Raw', color='lightgreen')\n",
    "    ax.plot(acc['timestamp'][:500], acc['acc_y_kf'][:500], label='Kalman Filtered', color='green', linewidth=1.5)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=0.3, color='orange', linestyle=':', alpha=0.7, label='Turn threshold')\n",
    "    ax.axhline(y=-0.3, color='orange', linestyle=':', alpha=0.7)\n",
    "    ax.set_title('Y-Axis: Lateral (Turning)', fontweight='bold')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Acceleration (g)')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "    # Magnitude\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(acc['timestamp'][:500], acc['acc_magnitude'][:500], color='purple', linewidth=1)\n",
    "    ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='Gravity baseline')\n",
    "    ax.set_title('Acceleration Magnitude', fontweight='bold')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('|Acceleration| (g)')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "    # Speed from GPS\n",
    "    if gps is not None:\n",
    "        ax = axes[1, 1]\n",
    "        ax.plot(gps['timestamp'], gps['speed'], color='navy', linewidth=1)\n",
    "        ax.set_title('Speed (from GPS)', fontweight='bold')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Speed (km/h)')\n",
    "        ax.axhline(y=gps['speed'].mean(), color='red', linestyle='--', alpha=0.5, label=f\"Mean: {gps['speed'].mean():.1f}\")\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(project_root / 'results' / 'figures' / 'raw_accelerometer_data.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print_success(\"Raw accelerometer visualization saved\")\n"
   ],
   "id": "5c91f90d468d2ea1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 1 Takeaway\n",
    "> **Raw sensor data is noisy but informative.** Kalman filtering smooths the signal while preserving sudden changes (events). The X-axis captures braking/acceleration, Y-axis captures turning. These patterns differ between NORMAL, DROWSY, and AGGRESSIVE driving.\n",
    "\n",
    "---\n"
   ],
   "id": "fa1efcae18530964"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Feature Engineering from Raw Sensors\n",
    "\n",
    "### What We're Doing\n",
    "We extract statistical features from raw sensor data for each trip.\n",
    "\n",
    "### Why Raw Features (NOT Pre-computed Scores)?\n",
    "\n",
    "| Approach | Problem |\n",
    "|----------|---------|\n",
    "| Pre-computed scores (score_braking, score_total) | **Circular logic** - scores use same heuristics as labels |\n",
    "| Behavioral ratios (ratio_normal, ratio_aggressive) | **Direct leakage** - ratios derived from labels |\n",
    "| **Raw sensor features** | âœ… Direct measurements, no leakage |\n",
    "\n",
    "### Features Extracted\n",
    "\n",
    "| Category | Features | Why Important |\n",
    "|----------|----------|---------------|\n",
    "| **Speed** | mean, std, max, min, change_mean, change_std | Driving intensity and variability |\n",
    "| **Course** | change_mean, change_std, change_max | Lane changes, turning frequency |\n",
    "| **Acceleration** | x/y means, stds, magnitude stats | Core driving behavior signal |\n",
    "| **Jerk** | x_std, y_std (acceleration derivative) | Driving smoothness indicator |\n",
    "| **Events** | brake_count, hard_brake_count, turn_count, sharp_turn_count | Discrete event summaries |\n"
   ],
   "id": "a789668a6025f0e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract features for sample trip\n",
    "sample_features = extract_raw_features(sample_trip['path'])\n",
    "print_header(\"SAMPLE TRIP FEATURES\", \"ðŸ“‹\")\n",
    "for k, v in list(sample_features.items())[:15]:\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "print(f\"  ... and {len(sample_features) - 15} more features\")\n"
   ],
   "id": "1a326792e6d46037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build full dataset from all trips\n",
    "print(\"\\nðŸ“Š Building dataset from all trips...\")\n",
    "raw_df = build_raw_dataset(trips)\n",
    "print(f\"Dataset shape: {raw_df.shape}\")\n",
    "print(f\"Features: {len([c for c in raw_df.columns if c not in ['driver', 'behavior', 'road_type']])}\")\n",
    "print(f\"Behaviors: {raw_df['behavior'].value_counts().to_dict()}\")\n",
    "\n",
    "# Save for reuse\n",
    "raw_df.to_csv(project_root / 'data' / 'processed' / 'uah_raw_features.csv', index=False)\n",
    "print_success(\"Raw features dataset saved!\")\n"
   ],
   "id": "4a6e2a7bfe33df46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 2 Takeaway\n",
    "> **We extracted 36 raw sensor features from GPS and accelerometer data.** These include speed statistics, acceleration patterns, jerk (smoothness), and event counts. No pre-computed scores are used, avoiding circular logic.\n",
    "\n",
    "---\n"
   ],
   "id": "305d7044fe7e8622"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### What We're Doing\n",
    "We visualize the extracted features to understand class separability and data quality.\n"
   ],
   "id": "6a1065d9420ce3a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "feature_cols = [c for c in raw_df.columns if c not in ['driver', 'behavior', 'road_type']]\n",
    "print(f\"ðŸ“Š Feature columns: {len(feature_cols)}\")\n"
   ],
   "id": "a4a9362fdab55b00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Class Distribution\n",
   "id": "9a82fded63f8bf12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot\n",
    "behavior_counts = raw_df['behavior'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "bars = axes[0].bar(behavior_counts.index, behavior_counts.values, color=colors, edgecolor='black')\n",
    "axes[0].set_title('Class Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Behavior')\n",
    "axes[0].set_ylabel('Count')\n",
    "for bar, count in zip(bars, behavior_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                 f'{count}\\n({100*count/len(raw_df):.1f}%)', ha='center', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(behavior_counts.values, labels=behavior_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Class Proportions', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "6df3831384dd0057"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Feature Distributions by Class\n",
   "id": "fe83686d2c192061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "key_features = ['speed_mean', 'speed_std', 'acc_magnitude_std', 'jerk_x_std',\n",
    "                'hard_brake_count', 'sharp_turn_count']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "for ax, feat in zip(axes.flatten(), key_features):\n",
    "    for behavior in ['NORMAL', 'DROWSY', 'AGGRESSIVE']:\n",
    "        data = raw_df[raw_df['behavior'] == behavior][feat]\n",
    "        ax.hist(data, alpha=0.5, label=behavior, bins=10)\n",
    "    ax.set_title(feat.replace('_', ' ').title(), fontweight='bold')\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'feature_distributions_classification.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "c5de92db8657ac8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Driver Behavior Distribution\n",
   "id": "99338c312aa32fda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "crosstab = pd.crosstab(raw_df['driver'], raw_df['behavior'])\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "crosstab.plot(kind='bar', ax=ax, color=['#2ecc71', '#f39c12', '#e74c3c'], edgecolor='black')\n",
    "ax.set_title('Trips per Driver by Behavior', fontweight='bold')\n",
    "ax.set_xlabel('Driver')\n",
    "ax.set_ylabel('Number of Trips')\n",
    "ax.legend(title='Behavior')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'driver_behavior_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "a3ed64f7fad75064"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4 Correlation Matrix\n",
   "id": "7a23b2e8fcdcb3ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "corr_matrix = raw_df[feature_cols].corr()\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'correlation_matrix_classification.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "7f583cd3e034878a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 3 Takeaway\n",
    "> **The dataset has 40 trips with relatively balanced classes.** Key features like speed_std, jerk, and hard_brake_count show visible separation between behaviors. Each driver has different behavior distributions, motivating driver-level splitting.\n",
    "\n",
    "---\n"
   ],
   "id": "1286e942bb33cc8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Data Preparation & Driver-Level Split\n",
    "\n",
    "### What We're Doing\n",
    "We prepare features and split data so **D6 is completely held out for testing**.\n",
    "\n",
    "### Why Driver-Level Split?\n",
    "\n",
    "| Split Strategy | Problem |\n",
    "|----------------|---------|\n",
    "| Random split | Model learns driver signatures, not behaviors |\n",
    "| K-Fold CV | Same issue - driver leakage |\n",
    "| **D6 Held-out** | âœ… Tests true generalization to NEW drivers |\n"
   ],
   "id": "ec37700ce940d3da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare features\n",
    "X = raw_df[feature_cols + ['driver']].copy()\n",
    "X[feature_cols] = X[feature_cols].fillna(0)\n",
    "y = raw_df['behavior'].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "classes = le.classes_\n",
    "\n",
    "# Split: D6 always held out\n",
    "X_train, X_test, y_train, y_test = split_by_driver(X, y_enc, test_drivers=['D6'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Dataset:\")\n",
    "print(f\"   Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"   Classes: {list(classes)}\")\n"
   ],
   "id": "e93c694dbcea0921"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 4 Takeaway\n",
    "> **D6 is completely held outâ€”the model has never seen this driver.** This simulates deploying to a new customer. Combined with stratified sampling, we have 80/20 train/test split.\n",
    "\n",
    "---\n"
   ],
   "id": "89451b548d76786f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Train All Classifiers\n",
    "\n",
    "### What We're Doing\n",
    "We train 15 different classifiers and compare their performance.\n",
    "\n",
    "### Models Compared\n",
    "\n",
    "| Category | Models |\n",
    "|----------|--------|\n",
    "| **Linear** | Logistic (L1, L2) |\n",
    "| **SVM** | Linear, RBF, Polynomial |\n",
    "| **KNN** | k=3, k=5 (weighted), k=7 (manhattan) |\n",
    "| **Trees** | Decision Tree, Extra Trees |\n",
    "| **Ensemble** | Random Forest, Gradient Boosting, AdaBoost |\n",
    "| **Neural** | MLP |\n",
    "| **Probabilistic** | Naive Bayes |\n"
   ],
   "id": "18ed3f793fe35dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "classifiers = {\n",
    "    # Linear\n",
    "    'Logistic (L2)': LogisticRegression(penalty='l2', max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    'Logistic (L1)': LogisticRegression(penalty='l1', solver='saga', max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "\n",
    "    # SVM\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42, class_weight='balanced'),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42, class_weight='balanced'),\n",
    "    'SVM (Poly)': SVC(kernel='poly', degree=3, random_state=42, class_weight='balanced'),\n",
    "\n",
    "    # KNN\n",
    "    'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
    "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
    "    'KNN (k=7)': KNeighborsClassifier(n_neighbors=7, weights='distance', metric='manhattan'),\n",
    "\n",
    "    # Trees\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42, class_weight='balanced'),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced'),\n",
    "\n",
    "    # Ensemble\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "\n",
    "    # Neural\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42),\n",
    "\n",
    "    # Probabilistic\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "}\n",
    "\n",
    "print_header(\"TRAINING ALL CLASSIFIERS\", \"ðŸŽ¯\")\n",
    "results = []\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    try:\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Train accuracy (overfitting check)\n",
    "        train_acc = accuracy_score(y_train, clf.predict(X_train_scaled))\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Train Acc': train_acc,\n",
    "            'Test Acc': acc,\n",
    "            'F1-Score': f1,\n",
    "            'Overfit': train_acc - acc\n",
    "        })\n",
    "        print(f\"âœ… {name}: Train={train_acc:.3f}, Test={acc:.3f}, F1={f1:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name}: {e}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(results).sort_values('Test Acc', ascending=False)\n"
   ],
   "id": "f6aafbcf848e1bb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 5 Takeaway\n",
    "> **All 15 classifiers trained successfully.** We track both train and test accuracy to detect overfitting. Ensemble methods typically lead, but interpretability may favor simpler models.\n",
    "\n",
    "---\n"
   ],
   "id": "8e51cd62b54b1e80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. CNN Classification (PyTorch)\n",
    "\n",
    "### What We're Doing\n",
    "We train a 1D Convolutional Neural Network to learn feature interactions automatically.\n",
    "\n",
    "### CNN Architecture\n",
    "| Layer | Configuration |\n",
    "|-------|--------------|\n",
    "| Conv1D | 64 filters, kernel=5 |\n",
    "| BatchNorm + ReLU + Pool | Normalize and reduce |\n",
    "| Conv1D | 128 filters, kernel=5 |\n",
    "| BatchNorm + ReLU + Pool | Normalize and reduce |\n",
    "| Dense | 128 units + Dropout |\n",
    "| Output | 3 classes (softmax) |\n"
   ],
   "id": "48ba15967b0b9204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print_header(\"CNN CLASSIFICATION (PyTorch)\", \"ðŸ§ \")\n",
    "\n",
    "cnn = CNNClassifier(\n",
    "    n_filters=64,\n",
    "    kernel_size=5,\n",
    "    hidden_size=128,\n",
    "    dropout=0.4,\n",
    "    epochs=150,\n",
    "    batch_size=8,\n",
    "    learning_rate=0.0005,\n",
    "    early_stopping_patience=20,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cnn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_cnn = cnn.predict(X_test_scaled)\n",
    "y_pred_cnn_enc = cnn.le_.transform(y_pred_cnn)\n",
    "acc_cnn = accuracy_score(y_test, y_pred_cnn_enc)\n",
    "f1_cnn = f1_score(y_test, y_pred_cnn_enc, average='weighted')\n",
    "train_acc_cnn = accuracy_score(y_train, cnn.le_.transform(cnn.predict(X_train_scaled)))\n",
    "\n",
    "print(f\"\\nðŸ“Š CNN Results:\")\n",
    "print(f\"   Train Accuracy: {train_acc_cnn:.4f}\")\n",
    "print(f\"   Test Accuracy:  {acc_cnn:.4f}\")\n",
    "print(f\"   F1-Score:       {f1_cnn:.4f}\")\n",
    "\n",
    "results.append({\n",
    "    'Model': 'CNN (PyTorch)',\n",
    "    'Train Acc': train_acc_cnn,\n",
    "    'Test Acc': acc_cnn,\n",
    "    'F1-Score': f1_cnn,\n",
    "    'Overfit': train_acc_cnn - acc_cnn\n",
    "})\n"
   ],
   "id": "f31f6b6c5bc40b86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot training history\n",
    "history = cnn.get_training_history()\n",
    "fig = plot_cnn_training_history(\n",
    "    history,\n",
    "    save_path=str(project_root / 'results' / 'figures' / 'cnn_learning_curves_classification.png')\n",
    ")\n",
    "print_success(\"CNN learning curves saved\")\n"
   ],
   "id": "5ddd99eba567fddb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 6 Takeaway\n",
    "> **CNN achieves competitive performance but may not outperform ensembles on this small dataset.** Deep learning typically needs more data. However, CNN is valuable for raw time-series (before aggregation).\n",
    "\n",
    "---\n"
   ],
   "id": "8c6f4f28c4a22f88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Model Comparison & Visualization\n",
   "id": "464a61c5a3884753"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Update comparison with CNN\n",
    "comparison_df = pd.DataFrame(results).sort_values('Test Acc', ascending=False)\n",
    "\n",
    "print_header(\"FINAL MODEL RANKING\", \"ðŸ†\")\n",
    "print(comparison_df.to_string(index=False))\n"
   ],
   "id": "d5d5f4d02b161475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Test Accuracy\n",
    "ax = axes[0]\n",
    "colors = plt.cm.RdYlGn(comparison_df['Test Acc'].values / comparison_df['Test Acc'].max())\n",
    "bars = ax.barh(range(len(comparison_df)), comparison_df['Test Acc'].values, color=colors)\n",
    "ax.set_yticks(range(len(comparison_df)))\n",
    "ax.set_yticklabels(comparison_df['Model'].values)\n",
    "ax.set_xlabel('Test Accuracy')\n",
    "ax.set_title('Model Comparison - Test Accuracy (D6 Held Out)', fontweight='bold')\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5)\n",
    "ax.set_xlim(0, 1)\n",
    "for i, acc in enumerate(comparison_df['Test Acc']):\n",
    "    ax.text(acc + 0.02, i, f'{acc:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# Train vs Test\n",
    "ax = axes[1]\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "ax.barh(x - width/2, comparison_df['Train Acc'].values, width, label='Train', color='#3498db')\n",
    "ax.barh(x + width/2, comparison_df['Test Acc'].values, width, label='Test', color='#e74c3c')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(comparison_df['Model'].values)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Train vs Test Accuracy (Overfitting Check)', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'classifier_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "best_model = comparison_df.iloc[0]\n",
    "print(f\"\\nðŸ¥‡ Best Model: {best_model['Model']}\")\n",
    "print(f\"   Test Accuracy: {best_model['Test Acc']:.4f}\")\n",
    "print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n"
   ],
   "id": "e5feb6ae434fe37c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 7 Takeaway\n",
    "> **Ensemble methods (Random Forest, Gradient Boosting) typically achieve highest accuracy.** The train vs test gap indicates some overfitting, expected with small datasets. Consider the accuracy-interpretability tradeoff for deployment.\n",
    "\n",
    "---\n"
   ],
   "id": "1deab34d6e57f58e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Confusion Matrix Analysis\n",
   "id": "e8c37fa6c5fcf835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "if best_model_name == 'CNN (PyTorch)':\n",
    "    y_pred_best = y_pred_cnn_enc\n",
    "else:\n",
    "    best_clf = classifiers[best_model_name]\n",
    "    best_clf.fit(X_train_scaled, y_train)\n",
    "    y_pred_best = best_clf.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes,\n",
    "            yticklabels=classes, ax=ax, cbar_kws={'label': 'Count'})\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name}\\n(D6 Held Out, Raw Features)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'confusion_matrix_classification.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=classes))\n"
   ],
   "id": "324a6568961adfe3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 8 Takeaway\n",
    "> **The confusion matrix reveals which classes are confused.** NORMAL vs DROWSY is often hardest to distinguish (subtle differences). AGGRESSIVE is usually well-separated due to distinctive harsh events.\n",
    "\n",
    "---\n"
   ],
   "id": "4547e108c38eeed8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Feature Importance Analysis\n",
   "id": "485079532fcde828"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced')\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print_header(\"TOP 15 MOST IMPORTANT FEATURES\", \"â­\")\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_n = importance_df.head(15).sort_values('Importance', ascending=True)\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(top_n)))\n",
    "ax.barh(range(len(top_n)), top_n['Importance'].values, color=colors)\n",
    "ax.set_yticks(range(len(top_n)))\n",
    "ax.set_yticklabels(top_n['Feature'].values)\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Top 15 Raw Sensor Features for Behavior Classification', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results' / 'figures' / 'feature_importance_classification.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "539d2872503e0a7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 9 Takeaway\n",
    "> **Top features align with physics: speed variance, jerk (smoothness), and event counts dominate.** These are direct sensor measurements with clear physical interpretation. No circular logic from pre-computed scores.\n",
    "\n",
    "---\n"
   ],
   "id": "c9ed0098112558ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Leave-One-Driver-Out Cross-Validation\n",
    "\n",
    "### Additional Validation\n",
    "We run LOGO CV to get per-driver accuracy estimates.\n"
   ],
   "id": "e118b60d7a99db7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_full = raw_df[feature_cols].fillna(0).values\n",
    "y_full = le.transform(raw_df['behavior'].values)\n",
    "groups = raw_df['driver'].values\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "top_models = {\n",
    "    'Logistic (L2)': LogisticRegression(penalty='l2', max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "}\n",
    "\n",
    "print_header(\"LEAVE-ONE-DRIVER-OUT CV\", \"ðŸ”¬\")\n",
    "for name, clf in top_models.items():\n",
    "    scores = cross_val_score(clf, X_full, y_full, cv=logo, groups=groups, scoring='accuracy')\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "    print(f\"  Per-driver: {[f'{s:.2f}' for s in scores]}\")\n"
   ],
   "id": "251e7eb7241856a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ“Œ Section 10 Takeaway\n",
    "> **LOGO CV gives the most realistic generalization estimate.** Each driver is held out once, simulating deployment to 6 new customers. Variance across drivers indicates which drivers are harder to classify.\n",
    "\n",
    "---\n"
   ],
   "id": "da84904ca6c4e871"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Summary & Key Takeaways\n",
    "\n",
    "### Model Performance (D6 Held Out, Raw Features)\n",
    "\n",
    "| Model | Train Acc | Test Acc | Notes |\n",
    "|-------|-----------|----------|-------|\n",
    "| **Logistic (L2)** | 1.00 | **0.75** | **Best - simple, interpretable** |\n",
    "| **Logistic (L1)** | 1.00 | **0.75** | Feature selection |\n",
    "| Random Forest | 1.00 | 0.75 | Ensemble, good accuracy |\n",
    "| Gradient Boosting | 1.00 | 0.63 | Overfits on small data |\n",
    "| SVM (RBF) | 0.94 | 0.63 | Non-linear boundaries |\n",
    "| KNN (k=5) | 1.00 | 0.50 | Instance-based |\n",
    "| CNN (PyTorch) | 0.85 | 0.50 | Needs more data |\n",
    "\n",
    "### Why Logistic Regression Wins\n",
    "\n",
    "1. **Small dataset**: With only 40 trips, complex models overfit\n",
    "2. **Good features**: Raw sensor features are discriminative for linear separation\n",
    "3. **L1/L2 regularization**: Prevents overfitting\n",
    "4. **Interpretable**: Clear coefficients explain predictions\n",
    "5. **Fast inference**: Production-ready\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "| Decision | Rationale | Impact |\n",
    "|----------|-----------|--------|\n",
    "| **Raw features only** | Avoid circular logic | Honest evaluation |\n",
    "| **D6 held-out** | Test new driver generalization | Production-realistic |\n",
    "| **Multiple models** | Find best tradeoff | Logistic wins |\n",
    "| **Train/Test tracking** | Detect overfitting | Robust models |\n",
    "\n",
    "### Recommendations for ABAX\n",
    "\n",
    "1. **Use Logistic Regression** for best accuracy AND interpretability\n",
    "2. **Use L1 regularization** for automatic feature selection\n",
    "3. **Collect more data** before using complex models (RF, CNN)\n",
    "4. **Always hold out complete drivers** for testing\n",
    "5. **Monitor train-test gap** for overfitting\n",
    "\n",
    "---\n",
    "\n",
    "**âœ… Classification Pipeline Complete**\n",
    "\n",
    "This notebook demonstrated the complete pipeline from raw sensors to behavior classification. Key finding: **Logistic Regression achieves the best test accuracy (75%)** on this small dataset, demonstrating that simpler models often generalize better when data is limited.\n"
   ],
   "id": "b4d556e850be7fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T21:46:49.378549Z",
     "start_time": "2025-12-28T21:46:49.366072Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3a8fc88c3c2db30a",
   "outputs": [],
   "execution_count": 33
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
